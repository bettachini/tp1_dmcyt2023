
@online{kamienkowski_curso_2023,
	title = {Curso: Data Mining en Ciencia y Tecnología},
	url = {https://datamining.dc.uba.ar/campus/course/view.php?id=37},
	author = {Kamienkowski, Juan A.},
	urldate = {2023-09-12},
	date = {2023},
	file = {Curso\: Data Mining en Ciencia y Tecnología:/home/vbettachini/storage/Zotero/storage/HYK85YW9/view.html:text/html},
}

@online{belitskaya_flower_2020,
	title = {Flower Color Images},
	url = {https://www.kaggle.com/datasets/olgabelitskaya/flower-color-images},
	abstract = {Flower Color Images, Set for Classification},
	author = {Belitskaya, Olga},
	urldate = {2023-09-12},
	date = {2020},
	langid = {english},
	file = {Snapshot:/home/vbettachini/storage/Zotero/storage/SGQIZ4NR/flower-color-images.html:text/html},
}

@online{taskesen_pca_2020,
	title = {{PCA} — clustimage clustimage documentation},
	url = {https://erdogant.github.io/clustimage/pages/html/Feature%20Extraction.html},
	author = {Taskesen, E.},
	urldate = {2023-09-12},
	date = {2020},
	file = {PCA — clustimage clustimage documentation:/home/vbettachini/storage/Zotero/storage/VBDD4LP4/Feature Extraction.html:text/html},
}

@online{noauthor_opencv_nodate,
	title = {{OpenCV}: Changing the contrast and brightness of an image!},
	url = {https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html},
	urldate = {2023-09-12},
	file = {OpenCV\: Changing the contrast and brightness of an image!:/home/vbettachini/storage/Zotero/storage/V3GCU5B3/tutorial_basic_linear_transform.html:text/html},
}

@online{noauthor_standard_1996,
	title = {A Standard Default Color Space for the Internet - {sRGB}},
	url = {https://www.w3.org/Graphics/Color/sRGB},
	titleaddon = {World Wide Web Consortium},
	urldate = {2023-09-12},
	date = {1996-11-05},
	file = {A Standard Default Color Space for the Internet - sRGB:/home/vbettachini/storage/Zotero/storage/76SJYU7X/sRGB.html:text/html},
}

@online{noauthor_opencv_nodate-1,
	title = {{OpenCV}: Image Filtering},
	url = {https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html},
	urldate = {2023-09-13},
	file = {OpenCV\: Image Filtering:/home/vbettachini/storage/Zotero/storage/CDS5JWIZ/group__imgproc__filter.html:text/html},
}

@online{noauthor_api_nodate,
	title = {{API} References — clustimage clustimage documentation},
	url = {https://erdogant.github.io/clustimage/pages/html/clustimage.clustimage.html#clustimage.clustimage.Clustimage.fit_transform},
	urldate = {2023-09-15},
	file = {API References — clustimage clustimage documentation:/home/vbettachini/storage/Zotero/storage/VT4V4ZZG/clustimage.clustimage.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2023-10-13},
	date = {2015-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:/home/vbettachini/storage/Zotero/storage/H8XF5PPH/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@online{team_keras_nodate,
	title = {Keras documentation: {VGG}16 and {VGG}19},
	url = {https://keras.io/api/applications/vgg/},
	shorttitle = {Keras documentation},
	abstract = {Keras documentation},
	author = {Team, Keras},
	urldate = {2023-10-13},
	langid = {english},
	file = {Snapshot:/home/vbettachini/storage/Zotero/storage/2MBSPZTK/vgg.html:text/html},
}

@article{koklu_classification_2021,
	title = {Classification of rice varieties with deep learning methods},
	volume = {187},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169921003021},
	doi = {10.1016/j.compag.2021.106285},
	abstract = {Rice, which is among the most widely produced grain products worldwide, has many genetic varieties. These varieties are separated from each other due to some of their features. These are usually features such as texture, shape, and color. With these features that distinguish rice varieties, it is possible to classify and evaluate the quality of seeds. In this study, Arborio, Basmati, Ipsala, Jasmine and Karacadag, which are five different varieties of rice often grown in Turkey, were used. A total of 75,000 grain images, 15,000 from each of these varieties, are included in the dataset. A second dataset with 106 features including 12 morphological, 4 shape and 90 color features obtained from these images was used. Models were created by using Artificial Neural Network ({ANN}) and Deep Neural Network ({DNN}) algorithms for the feature dataset and by using the Convolutional Neural Network ({CNN}) algorithm for the image dataset, and classification processes were performed. Statistical results of sensitivity, specificity, prediction, F1 score, accuracy, false positive rate and false negative rate were calcu­ lated using the confusion matrix values of the models and the results of each model were given in tables. Classification successes from the models were achieved as 99.87\% for {ANN}, 99.95\% for {DNN} and 100\% for {CNN}. With the results, it is seen that the models used in the study in the classification of rice varieties can be applied successfully in this field.},
	pages = {106285},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Computers and Electronics in Agriculture},
	author = {Koklu, Murat and Cinar, Ilkay and Taspinar, Yavuz Selim},
	urldate = {2023-10-16},
	date = {2021-08},
	langid = {english},
	file = {Koklu et al. - 2021 - Classification of rice varieties with deep learnin.pdf:/home/vbettachini/storage/Zotero/storage/TM684H65/Koklu et al. - 2021 - Classification of rice varieties with deep learnin.pdf:application/pdf},
}

@online{murat_kuklu_rice_2022,
	title = {Rice Image Dataset},
	url = {https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset},
	abstract = {Five different Rice Image Dataset. Arborio, Basmati, Ipsala, Jasmine, Karacadag},
	author = {{Murat Kuklu}},
	urldate = {2023-10-16},
	date = {2022},
	langid = {english},
	file = {Snapshot:/home/vbettachini/storage/Zotero/storage/TXR2FUXR/rice-image-dataset.html:text/html},
}

@article{knyazev_toward_2001,
	title = {Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method},
	volume = {23},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/S1064827500366124},
	doi = {10.1137/S1064827500366124},
	shorttitle = {Toward the Optimal Preconditioned Eigensolver},
	abstract = {We describe new algorithms of the locally optimal block preconditioned conjugate gradient ({LOBPCG}) method for symmetric eigenvalue problems, based on a local optimization of a three-term recurrence, and suggest several other new methods. To be able to compare numerically diﬀerent methods in the class, with diﬀerent preconditioners, we propose a common system of model tests, using random preconditioners and initial guesses. As the “ideal” control algorithm, we advocate the standard preconditioned conjugate gradient method for ﬁnding an eigenvector as an element of the null-space of the corresponding homogeneous system of linear equations under the assumption that the eigenvalue is known. We recommend that every new preconditioned eigensolver be compared with this “ideal” algorithm on our model test problems in terms of the speed of convergence, costs of every iteration, and memory requirements. We provide such comparison for our {LOBPCG} method. Numerical results establish that our algorithm is practically as eﬃcient as the “ideal” algorithm when the same preconditioner is used in both methods. We also show numerically that the {LOBPCG} method provides approximations to ﬁrst eigenpairs of about the same quality as those by the much more expensive global optimization method on the same generalized block Krylov subspace. We propose a new version of block Davidson’s method as a generalization of the {LOBPCG} method. Finally, direct numerical comparisons with the Jacobi–Davidson method show that our method is more robust and converges almost two times faster.},
	pages = {517--541},
	number = {2},
	journaltitle = {{SIAM} Journal on Scientific Computing},
	shortjournal = {{SIAM} J. Sci. Comput.},
	author = {Knyazev, Andrew V.},
	urldate = {2023-10-16},
	date = {2001-01},
	langid = {english},
	file = {Knyazev - 2001 - Toward the Optimal Preconditioned Eigensolver Loc.pdf:/home/vbettachini/storage/Zotero/storage/V3Q27Y2M/Knyazev - 2001 - Toward the Optimal Preconditioned Eigensolver Loc.pdf:application/pdf},
}
